{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Задание 1"
      ],
      "metadata": {
        "id": "G2qcGYN-Rv00"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bb0sCZIDIrF1",
        "outputId": "13cda0b2-1be7-40af-d701-1267a9b837ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import keras\n",
        "from nltk.corpus import brown\n",
        "\n",
        "nltk.download('brown')\n",
        "\n",
        "selected_categories = ['government', 'hobbies', 'humor', 'news']\n",
        "texts = []\n",
        "labels = []\n",
        "\n",
        "for category in selected_categories:\n",
        "    fileids = brown.fileids(categories=category)\n",
        "    for fileid in fileids:\n",
        "        texts.append(' '.join(brown.words(fileid)))\n",
        "        labels.append(category)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Преобразуем метки в числовой формат\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)"
      ],
      "metadata": {
        "id": "Cp90XdRqKP_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Токенизация текстов\n",
        "tokenizer = Tokenizer(num_words=10000)  # Ограничиваем словарь 10 000 слов\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Добавление padding\n",
        "max_sequence_length = 200  # Максимальная длина текста\n",
        "data = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')"
      ],
      "metadata": {
        "id": "-diOiZ1xKR8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Разделение данных\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, encoded_labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "dUVfqhcLKSIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "vocab_size = 10000  # Размер словаря\n",
        "embedding_dim = 128  # Размерность эмбеддингов\n",
        "lstm_units = 64\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "    LSTM(lstm_units),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "1BVKLhLOOA8z",
        "outputId": "92d2b5e9-b7de-4765-87c9-a4a46078dfab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучение модели\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=10, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG8mcjGKKXf3",
        "outputId": "b55914f7-34b8-49b9-a83c-bf8398346ea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - accuracy: 0.3107 - loss: 1.3780 - val_accuracy: 0.2500 - val_loss: 1.3591\n",
            "Epoch 2/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4570 - loss: 1.3173 - val_accuracy: 0.3750 - val_loss: 1.2766\n",
            "Epoch 3/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.3970 - loss: 1.2211 - val_accuracy: 0.3750 - val_loss: 1.2680\n",
            "Epoch 4/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5219 - loss: 1.1292 - val_accuracy: 0.3750 - val_loss: 1.2540\n",
            "Epoch 5/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6540 - loss: 0.9797 - val_accuracy: 0.4167 - val_loss: 1.2181\n",
            "Epoch 6/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7236 - loss: 0.8133 - val_accuracy: 0.4167 - val_loss: 1.1944\n",
            "Epoch 7/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8726 - loss: 0.6314 - val_accuracy: 0.4167 - val_loss: 1.1993\n",
            "Epoch 8/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8716 - loss: 0.4662 - val_accuracy: 0.4583 - val_loss: 1.3252\n",
            "Epoch 9/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9142 - loss: 0.3516 - val_accuracy: 0.4167 - val_loss: 1.3732\n",
            "Epoch 10/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9220 - loss: 0.3164 - val_accuracy: 0.4167 - val_loss: 1.2281\n",
            "Epoch 11/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9768 - loss: 0.2298 - val_accuracy: 0.3750 - val_loss: 1.1648\n",
            "Epoch 12/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9728 - loss: 0.1651 - val_accuracy: 0.5000 - val_loss: 1.1322\n",
            "Epoch 13/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9750 - loss: 0.1570 - val_accuracy: 0.3750 - val_loss: 1.2000\n",
            "Epoch 14/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0747 - val_accuracy: 0.4167 - val_loss: 1.3326\n",
            "Epoch 15/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0638 - val_accuracy: 0.5417 - val_loss: 1.1996\n",
            "Epoch 16/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0442 - val_accuracy: 0.5417 - val_loss: 1.3667\n",
            "Epoch 17/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0337 - val_accuracy: 0.6250 - val_loss: 1.1885\n",
            "Epoch 18/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0304 - val_accuracy: 0.4583 - val_loss: 1.4276\n",
            "Epoch 19/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0231 - val_accuracy: 0.5000 - val_loss: 1.6042\n",
            "Epoch 20/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0205 - val_accuracy: 0.5417 - val_loss: 1.6595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Оценка модели\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhMWAbCtKePw",
        "outputId": "696a25d4-f502-4d3e-ee69-61d8d8a9e9ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5417 - loss: 1.6595\n",
            "Test Accuracy: 0.5417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# Загрузка данных\n",
        "nltk.download('brown')\n",
        "selected_categories = ['government', 'hobbies', 'humor', 'news']\n",
        "texts = []\n",
        "labels = []\n",
        "\n",
        "for category in selected_categories:\n",
        "    fileids = brown.fileids(categories=category)\n",
        "    for fileid in fileids:\n",
        "        texts.append(' '.join(brown.words(fileid)))\n",
        "        labels.append(category)\n",
        "\n",
        "# Преобразуем метки в числовой формат\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Упорядочивание данных по длине текстов\n",
        "text_lengths = [len(seq) for seq in sequences]\n",
        "sorted_indices = np.argsort(text_lengths)\n",
        "sorted_sequences = [sequences[i] for i in sorted_indices]\n",
        "sorted_labels = [encoded_labels[i] for i in sorted_indices]\n",
        "\n",
        "# Разбиение на мини-блоки\n",
        "def create_mini_batches(sequences, labels, batch_size):\n",
        "    batches = []\n",
        "    batch_labels = []\n",
        "    current_batch = []\n",
        "    current_labels = []\n",
        "    max_len_in_batch = 0\n",
        "\n",
        "    for seq, label in zip(sequences, labels):\n",
        "        if len(current_batch) < batch_size:\n",
        "            current_batch.append(seq)\n",
        "            current_labels.append(label)\n",
        "            max_len_in_batch = max(max_len_in_batch, len(seq))\n",
        "        else:\n",
        "            padded_batch = pad_sequences(current_batch, maxlen=max_len_in_batch, padding='post')\n",
        "            batches.append(padded_batch)\n",
        "            batch_labels.append(np.array(current_labels))\n",
        "            current_batch = [seq]\n",
        "            current_labels = [label]\n",
        "            max_len_in_batch = len(seq)\n",
        "\n",
        "    if current_batch:\n",
        "        padded_batch = pad_sequences(current_batch, maxlen=max_len_in_batch, padding='post')\n",
        "        batches.append(padded_batch)\n",
        "        batch_labels.append(np.array(current_labels))\n",
        "\n",
        "    return batches, batch_labels\n",
        "\n",
        "batch_size = 32\n",
        "mini_batches, mini_batch_labels = create_mini_batches(sorted_sequences, sorted_labels, batch_size)\n",
        "\n",
        "vocab_size = 10000\n",
        "embedding_dim = 128\n",
        "lstm_units = 64\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max(text_lengths)),\n",
        "    LSTM(lstm_units),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Обучение модели на мини-блоках\n",
        "for epoch in range(30):\n",
        "    for batch, batch_labels in zip(mini_batches, mini_batch_labels):\n",
        "        model.train_on_batch(batch, batch_labels)\n",
        "\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"Epoch {epoch + 1}, Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEGgNqMebUkM",
        "outputId": "b4ee1cdf-980a-4d26-a936-0cf8600f7ba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Test Accuracy: 0.4167\n",
            "Epoch 2, Test Accuracy: 0.4167\n",
            "Epoch 3, Test Accuracy: 0.4583\n",
            "Epoch 4, Test Accuracy: 0.4583\n",
            "Epoch 5, Test Accuracy: 0.4583\n",
            "Epoch 6, Test Accuracy: 0.4583\n",
            "Epoch 7, Test Accuracy: 0.4583\n",
            "Epoch 8, Test Accuracy: 0.4583\n",
            "Epoch 9, Test Accuracy: 0.5000\n",
            "Epoch 10, Test Accuracy: 0.4583\n",
            "Epoch 11, Test Accuracy: 0.4583\n",
            "Epoch 12, Test Accuracy: 0.4583\n",
            "Epoch 13, Test Accuracy: 0.5000\n",
            "Epoch 14, Test Accuracy: 0.5000\n",
            "Epoch 15, Test Accuracy: 0.5417\n",
            "Epoch 16, Test Accuracy: 0.5417\n",
            "Epoch 17, Test Accuracy: 0.5833\n",
            "Epoch 18, Test Accuracy: 0.6250\n",
            "Epoch 19, Test Accuracy: 0.7083\n",
            "Epoch 20, Test Accuracy: 0.6250\n",
            "Epoch 21, Test Accuracy: 0.6250\n",
            "Epoch 22, Test Accuracy: 0.6250\n",
            "Epoch 23, Test Accuracy: 0.6667\n",
            "Epoch 24, Test Accuracy: 0.7500\n",
            "Epoch 25, Test Accuracy: 0.7917\n",
            "Epoch 26, Test Accuracy: 0.7500\n",
            "Epoch 27, Test Accuracy: 0.7917\n",
            "Epoch 28, Test Accuracy: 0.7500\n",
            "Epoch 29, Test Accuracy: 0.7500\n",
            "Epoch 30, Test Accuracy: 0.7083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Задание 2"
      ],
      "metadata": {
        "id": "qeNl3pIvR67l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('train_FD001.txt', sep=\" \", header=None)\n",
        "data = data.dropna(axis=1, how='all')\n",
        "\n",
        "# Группировка по engine_id\n",
        "grouped_data = data.groupby(0)"
      ],
      "metadata": {
        "id": "57hXD_doR5zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.iloc[:, 2:26]"
      ],
      "metadata": {
        "id": "mJpmS_DI37Pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rul = grouped_data[1].max()"
      ],
      "metadata": {
        "id": "-4JEfiAj4G40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['RUL'] = rul[data[0]].values - data[1]"
      ],
      "metadata": {
        "id": "xZo1DRsv4IQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.loc[:, X.nunique() > 1]  # Удаление константных колонок"
      ],
      "metadata": {
        "id": "mCxoJG7o4KWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_data = data.groupby(0).apply(lambda x: x.sort_values(by=[1])).reset_index(drop=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hoRpxgK4L1M",
        "outputId": "787a4cf1-29af-434b-d3d8-073d1671fc4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-77188d773f7e>:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  sorted_data = data.groupby(0).apply(lambda x: x.sort_values(by=[1])).reset_index(drop=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# Обрезка RUL\n",
        "data['RUL'] = data['RUL'].clip(upper=150)"
      ],
      "metadata": {
        "id": "TCMaMGcR4NR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_normalized, data['RUL'], test_size=0.2, random_state=42)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, input_shape=(X_train.shape[1], 1)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1GW_Yx-4Qrk",
        "outputId": "cd04ca94-5e99-4a7f-e786-17c3f7140591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 10379.1846 - mae: 91.5785 - val_loss: 4507.8018 - val_mae: 58.4598\n",
            "Epoch 2/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 4341.9321 - mae: 57.4926 - val_loss: 965.9366 - val_mae: 26.7212\n",
            "Epoch 3/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 1209.1276 - mae: 28.3811 - val_loss: 926.4371 - val_mae: 24.6302\n",
            "Epoch 4/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 921.1943 - mae: 24.7441 - val_loss: 812.6607 - val_mae: 22.9313\n",
            "Epoch 5/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 898.9640 - mae: 24.4302 - val_loss: 772.5686 - val_mae: 22.3843\n",
            "Epoch 6/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 919.0493 - mae: 24.7937 - val_loss: 778.7954 - val_mae: 22.2541\n",
            "Epoch 7/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 858.6594 - mae: 23.7354 - val_loss: 808.4528 - val_mae: 22.8913\n",
            "Epoch 8/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 864.2331 - mae: 23.8825 - val_loss: 737.9587 - val_mae: 21.7650\n",
            "Epoch 9/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 875.1159 - mae: 24.0476 - val_loss: 734.5630 - val_mae: 21.0651\n",
            "Epoch 10/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 820.2496 - mae: 23.3804 - val_loss: 721.6979 - val_mae: 21.5476\n",
            "Epoch 11/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 820.8209 - mae: 23.2994 - val_loss: 753.1270 - val_mae: 22.1517\n",
            "Epoch 12/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 806.0193 - mae: 23.1697 - val_loss: 695.1813 - val_mae: 20.9922\n",
            "Epoch 13/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 771.8156 - mae: 22.5717 - val_loss: 663.5663 - val_mae: 20.3338\n",
            "Epoch 14/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 766.6664 - mae: 22.3498 - val_loss: 1151.5940 - val_mae: 25.7593\n",
            "Epoch 15/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 755.4591 - mae: 22.2479 - val_loss: 746.2589 - val_mae: 20.6791\n",
            "Epoch 16/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 734.7478 - mae: 21.9015 - val_loss: 658.6375 - val_mae: 20.0248\n",
            "Epoch 17/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 733.3231 - mae: 21.8802 - val_loss: 647.7797 - val_mae: 20.0501\n",
            "Epoch 18/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 715.1259 - mae: 21.4288 - val_loss: 636.9625 - val_mae: 19.5428\n",
            "Epoch 19/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 715.7609 - mae: 21.3899 - val_loss: 640.2485 - val_mae: 19.5261\n",
            "Epoch 20/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 737.4766 - mae: 21.8149 - val_loss: 637.1947 - val_mae: 19.6871\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fd160462250>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Расчет метрик\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"RMSE: {rmse}\")\n",
        "print(f\"R²: {r2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S2TgFuX5Ty_",
        "outputId": "598cf298-a61d-46b0-ca1a-b10f1dfcaee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "RMSE: 25.242717032514424\n",
            "R²: 0.7388875484466553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X_normalized, data['RUL'], test_size=0.2, random_state=42)\n",
        "\n",
        "model_raw = Sequential()\n",
        "model_raw.add(LSTM(50, input_shape=(X_train_raw.shape[1], 1)))\n",
        "model_raw.add(Dropout(0.5))\n",
        "model_raw.add(BatchNormalization())\n",
        "model_raw.add(Dense(1))\n",
        "model_raw.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "#early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "model_raw.fit(X_train_raw, y_train_raw, epochs=10, batch_size=32, validation_data=(X_test_raw, y_test_raw))\n",
        "\n",
        "y_pred_raw = model_raw.predict(X_test_raw)\n",
        "\n",
        "# Расчет метрик\n",
        "rmse_raw = np.sqrt(mean_squared_error(y_test_raw, y_pred_raw))\n",
        "mae_raw = mean_absolute_error(y_test_raw, y_pred_raw)\n",
        "r2_raw = r2_score(y_test_raw, y_pred_raw)\n",
        "\n",
        "print(f\"RMSE (без упорядочивания): {rmse_raw}\")\n",
        "print(f\"MAE (без упорядочивания): {mae_raw}\")\n",
        "print(f\"R² (без упорядочивания): {r2_raw}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1NymY-eYS_h",
        "outputId": "cead724a-2330-4f48-edc9-830e6965feef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 10378.6914 - val_loss: 6821.0088\n",
            "Epoch 2/10\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4239.3799 - val_loss: 1778.2062\n",
            "Epoch 3/10\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 1224.3434 - val_loss: 923.7646\n",
            "Epoch 4/10\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 1002.7106 - val_loss: 829.2434\n",
            "Epoch 5/10\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 947.3810 - val_loss: 820.9564\n",
            "Epoch 6/10\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 925.9230 - val_loss: 771.2502\n",
            "Epoch 7/10\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 869.8957 - val_loss: 930.5582\n",
            "Epoch 8/10\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 841.2140 - val_loss: 795.3683\n",
            "Epoch 9/10\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 828.1335 - val_loss: 705.4767\n",
            "Epoch 10/10\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 794.9036 - val_loss: 669.2725\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "RMSE (без упорядочивания): 25.870303110087683\n",
            "MAE (без упорядочивания): 20.133052825927734\n",
            "R² (без упорядочивания): 0.7257425785064697\n"
          ]
        }
      ]
    }
  ]
}